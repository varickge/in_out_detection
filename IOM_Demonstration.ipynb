{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e93cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2023-3-7 Python-3.9.7 torch-1.13.1 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24256MiB)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from yoloDetect import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e0d14",
   "metadata": {},
   "source": [
    "#### Select a video for IoM visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58be1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw video path, on which you would like to visualize IoM \n",
    "\n",
    "video_path = ''# Put the path here\n",
    "capture = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fd54e",
   "metadata": {},
   "source": [
    "#### Initialize Yolo and create a holder for door bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bea82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2023-3-7 Python-3.9.7 torch-1.13.1 CUDA:1 (NVIDIA GeForce RTX 3090 Ti, 24256MiB)\n",
      "\n",
      "YOLOv5 ðŸš€ 2023-3-7 Python-3.9.7 torch-1.13.1 CUDA:1 (NVIDIA GeForce RTX 3090 Ti, 24256MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "YOLOv5 ðŸš€ 2023-3-7 Python-3.9.7 torch-1.13.1 CUDA:1 (NVIDIA GeForce RTX 3090 Ti, 24256MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n"
     ]
    }
   ],
   "source": [
    "device = '1'\n",
    "yolo = yoloDetect(device=select_device(f\"cuda:{device}\"))\n",
    "door_weights = 'runs/train/door_model/weights/best.pt'\n",
    "person_weights = 'yolov5s.pt'\n",
    "\n",
    "# Initializing door/person Yolo\n",
    "door_yolo = DetectMultiBackend(\n",
    "    weights=door_weights, device=select_device(f\"cuda:{device}\"))\n",
    "person_yolo = DetectMultiBackend( weights=person_weights, device=select_device(f\"cuda:{device}\"))\n",
    "\n",
    "door_bbox = np.zeros(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37037b4c",
   "metadata": {},
   "source": [
    "#### Run Yolos for door and person detection, compute IoM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea9af26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "door_bbox = None\n",
    "conf_thres = 0.4\n",
    "door_conf_thres = 0.6\n",
    "person_conf_thres = 0.5\n",
    "door_detect = False\n",
    "zone_thresh = 20\n",
    "line_thickness = 5\n",
    "person_bbox = np.zeros((0, 4))\n",
    "fig, ax = plt.subplots()\n",
    "ret, frame = capture.read()\n",
    "plt.ion() \n",
    "\n",
    "while ret:\n",
    "    img = frame.copy()\n",
    "    img = img[..., ::-1]\n",
    "    if not door_detect:\n",
    "        door_bbox = yolo(\n",
    "                    door_yolo,\n",
    "                    img,\n",
    "                    img_size=(416, 416),\n",
    "                    scale_with=(1080, 1920),\n",
    "                    conf_thres=door_conf_thres\n",
    "        )[:, 1:]\n",
    "        \n",
    "        if not len(door_bbox):\n",
    "            print(\"No detections in frame\")\n",
    "            ret, frame = capture.read()\n",
    "            continue\n",
    "        else:\n",
    "            door_detect=True\n",
    "            door_bbox = door_bbox[0]\n",
    "\n",
    "            x, y, x_w, y_h = (\n",
    "                door_bbox[0] - zone_thresh,\n",
    "                door_bbox[1],\n",
    "                door_bbox[2] + zone_thresh,\n",
    "                door_bbox[3] + zone_thresh,\n",
    "            )\n",
    "            x, x_w, y, y_h = round_box(x, x_w, y, y_h, imgsz=img.shape[:2])\n",
    "            crop_size = (y, y_h, x, x_w)\n",
    "            imgsz = (y_h - y, x_w - x)\n",
    "            p1_door = (int(door_bbox[0]), int(door_bbox[1]))\n",
    "            p2_door = (int(door_bbox[2]), int(door_bbox[3]))\n",
    "\n",
    "    if len(door_bbox):\n",
    "        # Engage person Yolo\n",
    "        bboxes = yolo(\n",
    "                    person_yolo,\n",
    "                    img,\n",
    "                    img_size=imgsz,\n",
    "                    crop_size=crop_size,\n",
    "                    scale_with=imgsz,\n",
    "                    conf_thres=person_conf_thres\n",
    "                ).astype(int)\n",
    "                \n",
    "        # Put door rectangles on frame \n",
    "        cv2.rectangle(\n",
    "                    img[..., ::-1],\n",
    "                    p1_door,\n",
    "                    p2_door,\n",
    "                    (0, 0, 255),\n",
    "                    thickness=line_thickness,\n",
    "                    lineType=cv2.LINE_AA,\n",
    "                )\n",
    "        person_bbox = bboxes[np.where(bboxes[:, 0] == 0)[0], 1:]\n",
    "        \n",
    "        if len(bboxes):\n",
    "            for bbox_person in person_bbox[:1]:\n",
    "        \n",
    "                bbox_person[1] += crop_size[0]\n",
    "                bbox_person[3] += crop_size[0]\n",
    "                bbox_person[0] += crop_size[2]\n",
    "                bbox_person[2] += crop_size[2]\n",
    "                \n",
    "                p1 = (int(bbox_person[0]), int(bbox_person[1]))\n",
    "                p2 = (int(bbox_person[2]), int(bbox_person[3]))\n",
    "                cv2.rectangle(img[..., ::-1], p1, p2, (255, 0, 0), thickness=line_thickness, lineType=cv2.LINE_AA)\n",
    "                iom = box_iom(torch.tensor([bbox_person]), torch.tensor([door_bbox]))\n",
    "                cv2.putText(\n",
    "                    img[..., ::-1],\n",
    "                    f\" IOM {round(iom.item(), 3)}\",\n",
    "                    (10, 200),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    2,\n",
    "                    (255,255,255),\n",
    "                    thickness=5,\n",
    "                    lineType=cv2.LINE_AA)\n",
    "    \n",
    "    # plotting the video for IoM visualization\n",
    "    ax.imshow(img)\n",
    "    plt.show()\n",
    "    fig.canvas.draw() \n",
    "    ax.cla()\n",
    "    \n",
    "    ret, frame = capture.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac47b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86adc95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
